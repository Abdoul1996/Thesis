{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ab5aacb9",
      "metadata": {
        "id": "ab5aacb9",
        "outputId": "b25d01c0-11af-44b5-d9fd-108b6463bc89"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "299c2e9a",
      "metadata": {
        "id": "299c2e9a"
      },
      "outputs": [],
      "source": [
        "path_to_data = \"/Users/abdoulabdillahi/Desktop/Thesis/Bio_project/200_samples_with_encoded.csv\"\n",
        "df = pd.read_csv(path_to_data)\n",
        "gene_cols = [c for c in df.columns if c.startswith(\"gene_\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e2ed4271",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Run accession</th>\n",
              "      <th>Ciprofloxacin_NS</th>\n",
              "      <th>gene_0</th>\n",
              "      <th>gene_1</th>\n",
              "      <th>gene_2</th>\n",
              "      <th>gene_3</th>\n",
              "      <th>gene_4</th>\n",
              "      <th>gene_5</th>\n",
              "      <th>gene_6</th>\n",
              "      <th>gene_7</th>\n",
              "      <th>...</th>\n",
              "      <th>gene_15619</th>\n",
              "      <th>gene_15620</th>\n",
              "      <th>gene_15621</th>\n",
              "      <th>gene_15622</th>\n",
              "      <th>gene_15623</th>\n",
              "      <th>gene_15624</th>\n",
              "      <th>gene_15625</th>\n",
              "      <th>gene_15626</th>\n",
              "      <th>gene_15627</th>\n",
              "      <th>gene_15628</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ERR4035837</td>\n",
              "      <td>1</td>\n",
              "      <td>['a' 't' 'g' 't' 'c' 't' 'a' 't' 'a' 'c' 'a' '...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>['a' 't' 'g' ... 't' 'g' 'a']</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>...</td>\n",
              "      <td>['-' '-' 'g' 'a' 't' 'g' 'a' 'a' 'a' 'a' 'a' '...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>['a' 't' 'g' ... 't' 'g' 'a']</td>\n",
              "      <td>['a' 't' 'g' ... 't' 'g' 'a']</td>\n",
              "      <td>['-' '-' '-' '-' '-' '-' 'c' 't' 'g' 't' 'a' '...</td>\n",
              "      <td>['a' 't' 'a' 't' 'c' 'c' 'c' 'a' 'g' 'a' 't' '...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ERR4034242</td>\n",
              "      <td>1</td>\n",
              "      <td>['a' 't' 'g' 't' 'c' 't' 'a' 't' 'a' 'c' 'a' '...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan ... nan nan nan]</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>...</td>\n",
              "      <td>['-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>['a' 't' 'g' ... 't' 'g' 'a']</td>\n",
              "      <td>['a' 't' 'g' ... 't' 'g' 'a']</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>['a' 't' 'g' 't' 'c' 'c' 'c' 'a' 'g' 'a' 't' '...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ERR4034731</td>\n",
              "      <td>1</td>\n",
              "      <td>['a' 't' 'g' 't' 'c' 't' 'a' 't' 'a' 'c' 'a' '...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan ... nan nan nan]</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan ... nan nan nan]</td>\n",
              "      <td>[nan nan nan ... nan nan nan]</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>['a' 't' 'g' 't' 'c' 'c' 'c' 'a' 'g' 'a' 't' '...</td>\n",
              "      <td>['a' 't' 'g' 'c' 'g' 'g' 'c' 't' 't' 'g' 'c' '...</td>\n",
              "      <td>['a' 't' 'g' 'c' 'a' 'g' 't' 't' 't' 'g' 't' '...</td>\n",
              "      <td>['a' 't' 'g' 't' 'c' 'g' 'g' 'c' 'c' 'a' 'c' '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ERR4036152</td>\n",
              "      <td>0</td>\n",
              "      <td>['a' 't' 'g' 't' 'c' 't' 'a' 't' 'a' 'c' 'a' '...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>['a' 't' 'g' ... 't' 'g' 'a']</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>['a' 't' 'g' ... 't' 'g' 'a']</td>\n",
              "      <td>['a' 't' 'g' ... 't' 'g' 'a']</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>['a' 't' 'g' 't' 'c' 'c' 'c' 'a' 'g' 'a' 't' '...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ERR4036469</td>\n",
              "      <td>0</td>\n",
              "      <td>['a' 't' 'g' 't' 'c' 't' 'a' 't' 'a' 'c' 'a' '...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan ... nan nan nan]</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>['a' 't' 'g' ... 't' 'g' 'a']</td>\n",
              "      <td>['a' 't' 'g' ... 't' 'g' 'a']</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>['a' 't' 'g' 't' 'c' 'c' 'c' 'a' 'g' 'a' 't' '...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "      <td>[nan nan nan nan nan nan nan nan nan nan nan n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 15631 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  Run accession  Ciprofloxacin_NS  \\\n",
              "0    ERR4035837                 1   \n",
              "1    ERR4034242                 1   \n",
              "2    ERR4034731                 1   \n",
              "3    ERR4036152                 0   \n",
              "4    ERR4036469                 0   \n",
              "\n",
              "                                              gene_0  \\\n",
              "0  ['a' 't' 'g' 't' 'c' 't' 'a' 't' 'a' 'c' 'a' '...   \n",
              "1  ['a' 't' 'g' 't' 'c' 't' 'a' 't' 'a' 'c' 'a' '...   \n",
              "2  ['a' 't' 'g' 't' 'c' 't' 'a' 't' 'a' 'c' 'a' '...   \n",
              "3  ['a' 't' 'g' 't' 'c' 't' 'a' 't' 'a' 'c' 'a' '...   \n",
              "4  ['a' 't' 'g' 't' 'c' 't' 'a' 't' 'a' 'c' 'a' '...   \n",
              "\n",
              "                                              gene_1  \\\n",
              "0  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "1  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "2  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "3  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "4  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "\n",
              "                                              gene_2  \\\n",
              "0  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "1  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "2  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "3  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "4  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "\n",
              "                                              gene_3  \\\n",
              "0  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "1  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "2  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "3  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "4  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "\n",
              "                                              gene_4  \\\n",
              "0  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "1  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "2  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "3  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "4  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "\n",
              "                                              gene_5  \\\n",
              "0  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "1  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "2  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "3  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "4  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "\n",
              "                          gene_6  \\\n",
              "0  ['a' 't' 'g' ... 't' 'g' 'a']   \n",
              "1  [nan nan nan ... nan nan nan]   \n",
              "2  [nan nan nan ... nan nan nan]   \n",
              "3  ['a' 't' 'g' ... 't' 'g' 'a']   \n",
              "4  [nan nan nan ... nan nan nan]   \n",
              "\n",
              "                                              gene_7  ...  \\\n",
              "0  [nan nan nan nan nan nan nan nan nan nan nan n...  ...   \n",
              "1  [nan nan nan nan nan nan nan nan nan nan nan n...  ...   \n",
              "2  [nan nan nan nan nan nan nan nan nan nan nan n...  ...   \n",
              "3  [nan nan nan nan nan nan nan nan nan nan nan n...  ...   \n",
              "4  [nan nan nan nan nan nan nan nan nan nan nan n...  ...   \n",
              "\n",
              "                                          gene_15619  \\\n",
              "0  ['-' '-' 'g' 'a' 't' 'g' 'a' 'a' 'a' 'a' 'a' '...   \n",
              "1  ['-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '...   \n",
              "2  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "3  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "4  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "\n",
              "                                          gene_15620  \\\n",
              "0  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "1  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "2  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "3  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "4  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "\n",
              "                                          gene_15621  \\\n",
              "0  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "1  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "2  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "3  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "4  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "\n",
              "                      gene_15622                     gene_15623  \\\n",
              "0  ['a' 't' 'g' ... 't' 'g' 'a']  ['a' 't' 'g' ... 't' 'g' 'a']   \n",
              "1  ['a' 't' 'g' ... 't' 'g' 'a']  ['a' 't' 'g' ... 't' 'g' 'a']   \n",
              "2  [nan nan nan ... nan nan nan]  [nan nan nan ... nan nan nan]   \n",
              "3  ['a' 't' 'g' ... 't' 'g' 'a']  ['a' 't' 'g' ... 't' 'g' 'a']   \n",
              "4  ['a' 't' 'g' ... 't' 'g' 'a']  ['a' 't' 'g' ... 't' 'g' 'a']   \n",
              "\n",
              "                                          gene_15624  \\\n",
              "0  ['-' '-' '-' '-' '-' '-' 'c' 't' 'g' 't' 'a' '...   \n",
              "1  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "2  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "3  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "4  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "\n",
              "                                          gene_15625  \\\n",
              "0  ['a' 't' 'a' 't' 'c' 'c' 'c' 'a' 'g' 'a' 't' '...   \n",
              "1  ['a' 't' 'g' 't' 'c' 'c' 'c' 'a' 'g' 'a' 't' '...   \n",
              "2  ['a' 't' 'g' 't' 'c' 'c' 'c' 'a' 'g' 'a' 't' '...   \n",
              "3  ['a' 't' 'g' 't' 'c' 'c' 'c' 'a' 'g' 'a' 't' '...   \n",
              "4  ['a' 't' 'g' 't' 'c' 'c' 'c' 'a' 'g' 'a' 't' '...   \n",
              "\n",
              "                                          gene_15626  \\\n",
              "0  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "1  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "2  ['a' 't' 'g' 'c' 'g' 'g' 'c' 't' 't' 'g' 'c' '...   \n",
              "3  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "4  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "\n",
              "                                          gene_15627  \\\n",
              "0  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "1  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "2  ['a' 't' 'g' 'c' 'a' 'g' 't' 't' 't' 'g' 't' '...   \n",
              "3  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "4  [nan nan nan nan nan nan nan nan nan nan nan n...   \n",
              "\n",
              "                                          gene_15628  \n",
              "0  [nan nan nan nan nan nan nan nan nan nan nan n...  \n",
              "1  [nan nan nan nan nan nan nan nan nan nan nan n...  \n",
              "2  ['a' 't' 'g' 't' 'c' 'g' 'g' 'c' 'c' 'a' 'c' '...  \n",
              "3  [nan nan nan nan nan nan nan nan nan nan nan n...  \n",
              "4  [nan nan nan nan nan nan nan nan nan nan nan n...  \n",
              "\n",
              "[5 rows x 15631 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a8cee6ce",
      "metadata": {
        "id": "a8cee6ce"
      },
      "outputs": [],
      "source": [
        "def join_genes(row):\n",
        "    toks = []\n",
        "    for g in gene_cols:\n",
        "        cell = row[g]\n",
        "        if pd.isna(cell): continue\n",
        "        if isinstance(cell, str) and cell.startswith(\"[\"):\n",
        "            seq = [t for t in cell.strip(\"[]\").split() if t.lower()!=\"nan\"]\n",
        "        else:\n",
        "            seq = list(cell)\n",
        "        toks.append(\"\".join(seq))\n",
        "    return \"\".join(toks)\n",
        "\n",
        "df[\"text\"]  = df.apply(join_genes, axis=1)\n",
        "df[\"label\"] = df[\"Ciprofloxacin_NS\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "92eea6ee",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    'a''t''g''t''c''t''a''t''a''c''a''g''a''a''c''...\n",
              "Name: text, dtype: object"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text'][:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "50ceda03",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['label'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93283b3b",
      "metadata": {
        "id": "93283b3b"
      },
      "outputs": [],
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        # squeeze batch dimension:\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4470389d",
      "metadata": {
        "id": "4470389d",
        "outputId": "ace9f6c8-d28a-4d6f-b131-e20d4bd60c95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "checkpoint = \"zhihan1996/DNABERT-2-117M\"\n",
        "tokenizer  = AutoTokenizer.from_pretrained(checkpoint, use_fast=False, trust_remote_code=True)\n",
        "model      = BertForSequenceClassification.from_pretrained(checkpoint, num_labels=2, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ef46e2",
      "metadata": {
        "id": "e3ef46e2"
      },
      "outputs": [],
      "source": [
        "dataset = SequenceDataset(df[\"text\"].tolist(),\n",
        "                          df[\"label\"].tolist(),\n",
        "                          tokenizer,\n",
        "                          max_length=512)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size   = len(dataset) - train_size\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=8, shuffle=True,\n",
        "    collate_fn=lambda batch: tokenizer.pad(batch, return_tensors=\"pt\")\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds, batch_size=16, shuffle=False,\n",
        "    collate_fn=lambda batch: tokenizer.pad(batch, return_tensors=\"pt\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab2d05d",
      "metadata": {
        "id": "3ab2d05d"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_loader) * 3  # num_epochs=3\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=total_steps//10,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a027ef8",
      "metadata": {
        "id": "9a027ef8"
      },
      "outputs": [],
      "source": [
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss, total_correct, total_count = 0.0, 0, 0\n",
        "    loop = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "    for batch in loop:\n",
        "        batch = {k:v.to(device) for k,v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # metrics\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct = (preds == batch[\"labels\"]).sum().item()\n",
        "        bs = batch[\"labels\"].size(0)\n",
        "\n",
        "        total_loss    += loss.item() * bs\n",
        "        total_correct += correct\n",
        "        total_count   += bs\n",
        "\n",
        "        loop.set_postfix({\n",
        "            \"loss\": f\"{loss.item():.4f}\",\n",
        "            \"batch_acc\": f\"{correct/bs:.2%}\"\n",
        "        })\n",
        "\n",
        "    avg_loss = total_loss / total_count\n",
        "    avg_acc  = total_correct / total_count\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "def eval_epoch():\n",
        "    model.eval()\n",
        "    total_loss, total_correct, total_count = 0.0, 0, 0\n",
        "    loop = tqdm(val_loader, desc=\"Evaluating\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for batch in loop:\n",
        "            batch = {k:v.to(device) for k,v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct = (preds == batch[\"labels\"]).sum().item()\n",
        "            bs = batch[\"labels\"].size(0)\n",
        "\n",
        "            total_loss    += loss.item() * bs\n",
        "            total_correct += correct\n",
        "            total_count   += bs\n",
        "\n",
        "            loop.set_postfix({\n",
        "                \"loss\": f\"{loss.item():.4f}\",\n",
        "                \"batch_acc\": f\"{correct/bs:.2%}\"\n",
        "            })\n",
        "\n",
        "    avg_loss = total_loss / total_count\n",
        "    avg_acc  = total_correct / total_count\n",
        "    return avg_loss, avg_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ef5a3d4",
      "metadata": {
        "id": "8ef5a3d4",
        "outputId": "d8370612-4118-458d-9188-41dac40399a3"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 3\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     val_loss,   val_acc   \u001b[38;5;241m=\u001b[39m eval_epoch()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[8], line 4\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m losses, preds, labels \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      5\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k:v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
            "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mSequenceDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 12\u001b[0m     enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# squeeze batch dimension:\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     item \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m enc\u001b[38;5;241m.\u001b[39mitems()}\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2867\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2865\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2866\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2867\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2869\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2977\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2956\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2957\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2975\u001b[0m     )\n\u001b[1;32m   2976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2977\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3052\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[1;32m   3025\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3040\u001b[0m \u001b[38;5;124;03m        method).\u001b[39;00m\n\u001b[1;32m   3041\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3043\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3044\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3045\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3050\u001b[0m )\n\u001b[0;32m-> 3052\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3055\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3072\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3073\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:615\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    613\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    614\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 615\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:541\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 541\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    553\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    555\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    565\u001b[0m ]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch()\n",
        "    val_loss,   val_acc   = eval_epoch()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"  Train â€” loss: {train_loss:.4f}, acc: {train_acc:.4%}\")\n",
        "    print(f\"  Val   â€” loss: {val_loss:.4f}, acc: {val_acc:.4%}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ecoli",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
